{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focused Concept Miner Command Line Interface\n",
    "# User Guide\n",
    "By Eric Zhou, Carnegie Mellon University Tepper School of Business\n",
    "\n",
    "Please contact me at ebzhou@tepper.cmu.edu for any feedback or inquiries.\n",
    "\n",
    "Version: September 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "We present the Focused Concept Miner (FCM) Command Line Interface (CLI), an exploratory tool which operationalizes an interpretable, deep learning-based text mining algorithm such that managers and researchers can accessibly analyze high-resolution unstructured data. This is specifically designed to be accessible for managers and researchers with or without a technical background. For more information about FCM CLI, please visit [FCMiner](http://www.fcminer.com/).\n",
    "\n",
    "In this guide, we provide a step-by-step guide on setup, features, implementation, and troubleshooting of FCM CLI. We also discuss a general approach to explore data and filter results to obtain meaningful insights using FCM's __Grid Search__ and __Visualize__ features.\n",
    "\n",
    "Becoming comfortable with the features and processes we discuss in the following sections will enable you to rapidly extract insights from unstructured data in a fraction of the time that involved, multi-step approaches require. We encourage you to experiment with creative applications to glean insights and develop novel hypotheses from rich unstructured data.\n",
    "\n",
    "This document covers the following:\n",
    "\n",
    "1. Introduction & Caveats\n",
    "\n",
    "2. Installation\n",
    "\n",
    "3. Commands & Features\n",
    "\n",
    "4. Demonstration\n",
    "\n",
    "5. Closing Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Caveats\n",
    "Before we begin, we'd like to present several caveats.\n",
    "\n",
    "First, FCM is a purely data-driven exploratory tool that is correlational in nature. Thus, we do not make any causal claims about the results.\n",
    "\n",
    "Additionally, all machine learning models are noisy representations of reality and could be faulty. Patterns identified by FCM should be, at most, interpreted as insights into the model, not reality. Users must investigate further to validate potential patterns as informative of real-world phenomena and to make causal claims.\n",
    "\n",
    "Consequently, users should practice trial and error to achieve the best results with FCM. This involves leveraging two of its core features, **Grid Search** and **Visualize**, to validate patterns found by FCM using human judgement and domain expertise.\n",
    "\n",
    "That being said, trial and error when tuning hyperparameters and filtering results can be time-consuming and even frustrating as with most deep learning frameworks. Users must perform due diligence to arrive at good results. Refer to [this paper](https://arxiv.org/abs/1206.5533 ) for hyperparameter tuning tips and tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Getting Started: Installing Python\n",
    "FCM CLI requires a working installation of Python 3.6 or greater. Available Python distributions can be found [here](https://www.python.org/downloads/windows/). Alternative methods of installing Python include via the [Anaconda](https://www.anaconda.com/products/individual)/[Miniconda](https://docs.conda.io/en/latest/miniconda.html) distribution which will include Python 3.6 along with other scientific computing libraries.\n",
    "\n",
    "Download the appropriate Python distribution and complete the installation process.\n",
    "\n",
    "If you would prefer a more user-friendly interface for running FCM than the system command line, consider installing an integrated development environment (IDE). A couple good options include:\n",
    "\n",
    "• [Visual Studio](https://code.visualstudio.com/)\n",
    "\n",
    "• [PyCharm](https://www.jetbrains.com/pycharm/)\n",
    "\n",
    "We offer two installation options, either via Command Line/terminal or in Google Colab. If you have a machine with adequate hardware (GPU, high RAM) and prefer to use an IDE to run FCM or will simply use the Command Line, follow the steps outlined in Section 2.2.\n",
    "\n",
    "Otherwise, proceed to Section 2.3 to set up FCM in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 FCM CLI Setup: Command Line\n",
    "To setup the FCM CLI, we must first clone the repository from GitHub to your machine. The requisite FCM CLI files can be obtained [here](https://github.com/cygit/fcm). Click the green button \"Code\" and \"Download ZIP\". Save and extract the ZIP to an accessible location on your computer.\n",
    "\n",
    "![github](img/2.2_github.png)\n",
    "\n",
    "If you have access to GitHub desktop, you can clone the respository to your computer directly from GitHub via “Open with GitHub Desktop” or with the command: \n",
    "    \n",
    "    git clone git@github.com:ecfm/fcm_cli.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained the source files, we must install and activate the environment. Follow these steps:\n",
    "\n",
    "1. Open up your command line by opening the Start Menu and typing in \"cmd\". In the command line, you will see at the start of each line your current working directory (i.e. `/mnt/c/Users/name`). We must navigate to the fcm_cli directory that we extracted earlier.\n",
    "\n",
    "    ![cwd](img/2.2_cwd.png)\n",
    "\n",
    "\n",
    "2. Find the fcm_cli folder path in File Explorer, and copy the full path. In the example below, we will copy `C:\\Users\\name\\Documents\\GitHub\\fcm_cli`\n",
    "    \n",
    "    ![directory](img/navigate.png)\n",
    "3. In the command line, execute the following command with the relative path. Given the example path, the input should appear as:\n",
    "\n",
    "    ```bash\n",
    "    cd C:\\Users\\name\\Documents\\GitHub\\fcm_cli\n",
    "    ```\n",
    "    \n",
    "    Your new working directory should appear as `C:\\Users\\name\\Documents\\GitHub\\fcm_cli` in the command line.\n",
    "\n",
    "    ![cd](img/2.2_cd.png)\n",
    "4. Once you have navigated to the fcm_cli directory, execute the following commands in the command line to create a virtual environment and install FCM along with its required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd FCM_CLI_PATH # change the path here with your fcm_cli directory\n",
    "!python -m venv env\n",
    "!source env/bin/activate\n",
    "%cd src\n",
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **NOTE**: You must include the period at the end of `pip install --editable .`, otherwise the installation will fail!\n",
    "\n",
    "The first time you install FCM CLI, you must create a Python virtual environment and install the required packages. This may take several minutes.\n",
    "\n",
    "From hereafter, each time you start a new command line instance, you must navigate to the fcm_cli directory as above and execute the commands:\n",
    "```bash\n",
    "        source env/bin/activate\n",
    "        cd src\n",
    "```\n",
    "\n",
    "At this point, you should have run the following commands and are in the process of installing FCM CLI as seen below:\n",
    "\n",
    "![install](img/2.2_install.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you'd like to use a conda environment, execute the following to create a virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd FCM_CLI_PATH # change the path here with your fcm_cli directory\n",
    "!conda create --name fcm python=3.6\n",
    "!activate fcm\n",
    "%cd src\n",
    "!conda install --name fcm --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: There have been instances where the installation fails due to certain packages that can not be retrieved. Try rerunning `pip install editable .`. If this doesn't work, try installing the packages below individually using `pip install PACKAGE_NAME`\n",
    "\n",
    "The required packages are:\n",
    "- click\n",
    "- django\n",
    "- nltk\n",
    "- numpy\n",
    "- pandas\n",
    "- pyLDAvis\n",
    "- scipy\n",
    "- sklearn\n",
    "- torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Once the installation is complete, ensure that FCM CLI is working properly by using the __Help__ command. Execute the following:\n",
    "            \n",
    "        fcm --help\n",
    "            \n",
    "    You should see a prompt as follows:\n",
    "\n",
    "    ![Help Prompt](img/2.2_help.png)\n",
    "\n",
    "    Congratulations - your FCM CLI is now ready! In Section 3, we will discuss the available commands and features and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 RECOMMENDED: Setup in Google Colab\n",
    "\n",
    "If you do not have access to device with a GPU or adequate processing power, consider using Google Colab to run FCM CLI. Colab provides access to powerful GPUs and high-RAM environments to run Python code at no cost. The setup is even simpler as well!\n",
    "\n",
    "To setup FCM CLI in Google Colab, follow thes steps in [Colab Guide](./fcm_cli_colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Commands & Features\n",
    "\n",
    "Currently, FCM CLI has three core commands:\n",
    "\n",
    "1. __Train__: given a dataset and output directory, train FCM using a set of default hyperparameters.\n",
    "\n",
    "2. __Grid search__: given a configuration file, train FCM with user-defined hyperparameters.\n",
    "\n",
    "3. __Visualize__: display grid search results with filtering sliders to evaluate outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train\n",
    "\n",
    "The __Train__ command will train FCM on the specified dataset with default hyperparameters and save the outputs in the user-specified folder.\n",
    "\n",
    "To execute __Train__, enter the following command in the command line:\n",
    "\n",
    "`fcm train DATASET_NAME OUTPUT_DIRECTORY`\n",
    "\n",
    "The user specifies the `DATASET_NAME` exactly as found in the directory: `fcm_cli/src/dataset`. If `DATASET_NAME` is 'csv', user must also specify three options `--csv-path`(path to the csv file), `--csv-text` (column name of the text field in the csv file), and `--csv-label` (column name of the label field in the csv file). Results will be saved in the user-specified `OUTPUT_DIRECTORY`.\n",
    "\n",
    "The command line should appear as:\n",
    "\n",
    "![train](img/3.1_train.png)\n",
    "\n",
    "While this command is running, logs will be saved in `OUTPUT_DIRECTORY/fcm.log`.\n",
    "\n",
    "Metrics from each epoch will be written in `OUTPUT_DIRECTORY/train_metrics.txt`.\n",
    "\n",
    "Concept words will be saved in `OUTPUT_DIRECTORY/concept/epoch*.txt` for each epoch.\n",
    "\n",
    "The state of the model will be saved every 10 epochs and at the last epoch in `OUTPUT_DIRECTORY/model/epoch*.pytorch`.\n",
    "\n",
    "Concept distributions will be saved every 10 epochs as well in `OUTPUT_DIRECTORY/model/epoch*_train_doc_concept_probs.npy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Grid Search\n",
    "\n",
    "__Grid Search__ trains FCM on all combinations of hyperparameters in the user-specific search space in the configuration file associated with the dataset. To run __Grid Search__, execute the following in command line:\n",
    "\n",
    "`fcm grid-search ../configs/CONFIG_FILE`\n",
    "\n",
    "Where `CONFIG_FILE` is the exact name of the configuration file that you'd like to run. Once you run the command, you should see the below prompt in your command line. FCM CLI will load in the data and inform you of the number of combinations in the search space. \n",
    "\n",
    "![gridsearch](img/3.2_gridsearch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training will then begin for each of the combinations until completion or the process is interrupted. The process can be resumed with the same command above and FCM will continue processing at the last configuration.\n",
    "\n",
    "Grid search results will be saved to the directory according to “out_dir” in the configuration. The configuration file also contains all of the hyperparameters that can be changed by the user.\n",
    "\n",
    "The configuration files are stored in ..fcm_cli/configs and can be edited directly in NotePad or most IDEs. See below for an example configuration file open in NotePad. Generally, you should only adjust 'dataset_params' and 'fcm_params'. A brief description of the hyperparameters can be found at the end of this section.\n",
    "\n",
    "![config](img/3.2_config.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each grid search run, there will be directories named with a `RUN_ID`, the hash value of each set of hyperparameters being searched, i.e. a grid search result of dataset `prosper_loan` with `OUT_DIR=\"run0\"` and `RUN_ID=\"0f735f978246aa65aa1806299869978c\"`, the results are located in . `../grid_search/prosper_loan/run0/0f735f978246aa65aa1806299869978c`.\n",
    "\n",
    "Within each directory `../grid_search/DATASET_NAME/OUT_DIR/RUN_ID/` , you will find similar files as described above in __Train__.\n",
    "\n",
    "These include a log file `fcm.log`, a metrics file `train_metrics.txt`, concept words `concept/epoch*.txt`., saved models `model/epoch*.pytorch`, concept distributions `model/epoch*_train_doc_concept_probs.npy`, and the best metrics of each hyperparameter configuration `results.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIPS\n",
    "\n",
    "While grid search is running, do not open the results.csv - this may force the process to terminate. However, you may still examine the files found in specific configuration directories once the configuration is done training.\n",
    "\n",
    "We also advise that users run smaller search sets (less hyperparameter combinations), especially on larger datasets as it may take a long time to train all configurations. Wait for the message “Grid search complete!” before proceeding to view the results.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Descriptions\n",
    "\n",
    "1. __dataset__: name of the dataset to be used; the dataset must be defined in `fcm_cli/src/dataset/` and be a subclass of base_dataset.py\n",
    "2. __csv-path__: path to the csv file. Only needed if __dataset__ is 'csv'\n",
    "3. __csv-text__: column name of the text field in the csv file. Only needed if __dataset__ is 'csv'\n",
    "4. __csv-label__: column name of the label field in the csv file. Only needed if __dataset__ is 'csv'\n",
    "5. __gpus__: list of GPU devices if CUDA is available; ignored if no CUDA\n",
    "6. __max_threads__: max number of parallel threads to run grid search\n",
    "7. __out_dir__: the directory to save grid search output files\n",
    "\n",
    "#### Dataset parameters: hyperparameters for loading the dataset\n",
    "1. __window_size__: context window size for training word embeddings\n",
    "2. __min_df__: minimum document frequency of vocabulary\n",
    "3. __max_df__: maximum document frequency of vocabulary\n",
    "\n",
    "#### FCM parameters: hyperparameters for creating new FCM instances\n",
    "1. __embed_dim__: size of each word/concept embedding vector\n",
    "2. __nnegs__: number of negative context words to be sampled during the training of word embeddings\n",
    "3. __nconcepts__: number of concepts to be extracted\n",
    "4. __lam__: Dirichlet loss (L_dir) weight; the higher, the more sparse the concept distribution of each document.\n",
    "5. __rho__: prediction loss (L_clf) weight; the higher, the more the model focuses on prediction accuracy.\n",
    "6. __eta__: diversity loss (L_div) weight; the higher, the more different are concepts vectors from each other\n",
    "7. __inductive__: whether to use neural network to inductively predict concept weights of each document or use a concept weights embedding\n",
    "8. __inductive_dropout__: dropout rate of the inductive neural network\n",
    "9. __hidden_size__: size of the hidden layers in the inductive neural network\n",
    "10. __num_layers__: number of layersi n the inductive neural network\n",
    "\n",
    "#### Fit parameters: hyperparameters for training FCM\n",
    "1. __lr__: learning rate\n",
    "2. __nepochs__: number of training epochs\n",
    "3. __pred_only_epochs__: number of epochs optimized with prediction loss only\n",
    "4. __batch_size__: number of training examples per iteration\n",
    "5. __grad_clip__: maximum gradient magnitude; gradients will be clipped within the range \\[-grad_clip, grad_clip\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Visualize\n",
    "The __Visualize__ command generates an interactive interface where users can view all configuration results in a single grid search run and filter results on term frequency, document frequency, and FREX exclusivity.\n",
    "\n",
    "Execute __Visualize__ by entering the command while specifying the grid search result path of a specific grid search configuration:\n",
    "\n",
    "`fcm visualize ../grid_search/DATASET_NAME/OUT_DIR`\n",
    "\n",
    "In the image below, `DATASET_NAME=\"prosper_loan\"` and `OUT_DIR=\"test\"`, executing the command \n",
    "`fcm visualize ../grid_search/prosper_loan/test`\n",
    "will result in the following prompt where you must copy the development server address and paste in in your web browser WHILE appending '/viewer/' to the address to navigate to the visualization interface. The address should thus appear as: http://127.0.0.1:8000/viewer/\n",
    "\n",
    "![address](img/3.3_visualize_address.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf Filters & FREX\n",
    "\n",
    "The three filtering features are based on the _term frequency-inverse document frequency (tf-idf)_ mechanism which measures the importance of words in a collection of texts. The mechanics of tf-idf and FREX are explained in detail below.\n",
    "\n",
    "### Basic Intuition & Rules of Thumb\n",
    "For the sake of brevity, the general intuition is as follows:\n",
    "\n",
    "1. We want to filter out words that are either too frequent or too rare such that they are uninformative. Thus, we want to restrict the maximum and minimum of tf and df range.\n",
    "\n",
    "2. If you want to extract words that are more exclusive to a topic, increase the FREX exclusivity. Alternatively, if you want to extract more frequent words, decrease the FREX exclusivity.\n",
    "\n",
    "### How tf-idf and FREX Work\n",
    "_Term frequency (tf)_ is the count of occurences a term appears in a document.\n",
    "\n",
    "_Document frequency (df)_ is the count of occurrences a term appears in a set of documents. Given that structural terms (i.e. articles, prepositions) tend to appear frequently in documents, such high frequency but often less informative terms should have less weight in computing term importance. __NOTE__: The df filter in __Visualize__ only applies to obtained concept words, not the entire vocabulary from the texts. This differs from __Grid Search__ where the min-max df parameters apply to the entire vocabulary.\n",
    "\n",
    "The _inverse document frequency (idf)_ addresses this by scaling down the importance of frequent terms and scales up the importance of infrequent terms. Intuitively, idf will be low for the most frequent words in a document and high for less frequent, context-specific words.\n",
    "\n",
    "In accord with tf-idf, _FREX exclusivity_ is used to find words that are frequently found in and are exclusive to a topic. FREX strikes a balance between words that occur frequently and are exclusive such that extracted words are neither too frequent to simply discuss a topic nor are they so rare that they are uninformative. FREX is higher if words are more exclusive to each topic and is lower when more frequent words are desired.\n",
    "\n",
    "Results will change real-time as the user adjust the filter sliders, or updating the input boxes directly. We encourage users to experiment with the filtering features to better understand the coherence and recall of results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filters sliders will appear as...\n",
    "\n",
    "![filter](img/visualize_filters.png)\n",
    "\n",
    "... And visualized results will appear as the image below. λ, η, and ρ correspond to __lam__, __rho__, and __eta__ in the grid search configuration and the same notations in the FCM paper. Coherence scores are calculated with [Gensim Library](https://radimrehurek.com/gensim/models/coherencemodel.html). Specifically, [UMass Measure](http://qpleple.com/topic-coherence-to-evaluate-topic-models/) is used. Dir __L__, div __L__, clf __L__,  correspond to Dirichlet loss (L_dir), diversity loss (L_div), and prediction loss (L_clf) in the FCM paper. Total __L__ is the sum of these losses. `Train AUC` and `Test AUC` are the AUC of the model on the training set and the test set.\n",
    "\n",
    "![viz](img/3.3_visualization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Implementing Your Own Dataset\n",
    "\n",
    "You are also able to implement your own dataset by creating a Python script under fcm_cli/src/dataset which pre-processes the data for FCM. Popular data formats like comma/tab separated value (csv and tsv) are easily implemented. \n",
    "\n",
    "The dataset script must be a subclass of base_dataset.py and implement the load_data method. Refer to the other datasets such as prosper_loan.py for an example. Below, we provide a basic framework  based on prosper_loan.py that the dataset script should follow. For ease of implementation, feel free to copy prosper_loan.py and edit as needed.\n",
    "\n",
    "__NOTE__: FCM currently only supports binary classification. Ensure that your labels are binary before passing the data to FCM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# Read in the csv as a Pandas dataframe\n",
    "x_df = pd.read_csv(os.path.join(DATA_DIR, \"prosper_loan.csv\"))\n",
    "\n",
    "# Fill any empty fields in the text column and create a list of the texts\n",
    "x_df['Description'] = x_df['Description'].fillna('')\n",
    "documents = x_df['Description'].tolist()\n",
    "\n",
    "# Create numpy array of labels as binary variables\n",
    "labels = x_df['LoanStatus'].isin(['Paid', 'Defaulted (PaidInFull)', 'Defaulted (SettledInFull)']).astype(int).to_numpy()\n",
    "\n",
    "# Drop the X and Y variables along with any extra columns and create a numpy array for remaining explanatory variables\n",
    "x_df = x_df.drop(columns=['Key', 'LoanStatus', 'Description'])\n",
    "expvars = x_df.to_numpy()\n",
    "\n",
    "# Create the training and test sets. Test ratio is 0.15 by default\n",
    "doc_train, doc_test, y_train, y_test, expvars_train, expvars_test =\\ train_test_split(documents, labels, expvars, test_size=TEST_RATIO)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "# Use this exact code to implement base_dataset.py and load_data; only change the dataset name as indicated by the notes\n",
    "class ProsperDataset(BaseDataset): # NOTE: change dataset name\n",
    "    def __init__(self):\n",
    "        super().__init__(doc_train, doc_test, y_train, y_test, expvars_train, expvars_test)\n",
    "\n",
    "    def get_data_filename(self, params):\n",
    "        window_size = params[\"window_size\"]  # context window size\n",
    "        vocab_size = params[\"vocab_size\"]  # max vocabulary size\n",
    "        min_df = params.get(\"min_df\", MIN_DF)  # min document frequency of vocabulary, defaults to MIN_DF\n",
    "        max_df = params.get(\"max_df\", MAX_DF)  # max document frequency of vocabulary, defaults to MAX_DF\n",
    "        return os.path.join(DATA_DIR, \"prosper_w%d_v%d_min%.0E_max%.0E.pkl\" % (window_size, vocab_size, min_df, max_df)) # NOTE: change dataset name\n",
    "\n",
    "    def load_data(self, params):\n",
    "        print(\"Loading data...\")\n",
    "        window_size = params[\"window_size\"]  # context window size\n",
    "        vocab_size = params[\"vocab_size\"]  # max vocabulary size\n",
    "        min_df = params.get(\"min_df\", MIN_DF)  # min document frequency of vocabulary, defaults to MIN_DF\n",
    "        max_df = params.get(\"max_df\", MAX_DF)  # max document frequency of vocabulary, defaults to MAX_DF\n",
    "\n",
    "        vectorizer = CountVectorizer(tokenizer=tokenize, stop_words='english', min_df=min_df, max_df=max_df,\n",
    "                                     max_features=vocab_size)\n",
    "        return self.get_data_dict(self.get_data_filename(params), vectorizer, window_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Demonstration\n",
    "\n",
    "In this section, we walk through a demonstration of FCM CLI on a peer-to-peer lending dataset titled prosper_loan within the repository.\n",
    "\n",
    "Before proceeding, ensure that this iPython Notebook is located in the fcm_cli base directory. This is required for the following commands to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Dataset\n",
    "\n",
    "The data contains all loan requests that were funded on the crowdfunding platform Prosper between April 2007 and October 2008, consisting of 122,479 total listings. The data includes (1) a loan application description (X-variable), (2) relevant personal information about the applicant and loan (credit grade, lender rate, etc.) (explanatory variables), and (3) a binary label indicating paid loans (Y-variable).\n",
    "\n",
    "See prosper_loan.csv in fcm_cli/data/prosper_loan for the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Initial Setup\n",
    "\n",
    "If you haven't set up FCM CLI yet, run the setup commands to create a virtual environment, activate it, navigate to the appropriate directory, and install the requisite packages. Execute the commands below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create virtual environment in base directory\n",
    "!python3 -m venv fcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Activate virtual environment.\n",
    "!source fcm/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Navigate to src\n",
    "%cd src "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install FCM CLI in the virtual environment. This will take several minutes the first time you run it\n",
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you run into any issues with the setup in this notebook, you may run the commands in Command Line/terminal.\n",
    "\n",
    "Also, you can set up your Google Colab notebook as explained in Section 2.3 and follow along. We also provide fcm_cli_colab.ipynb which you can upload to Colab and begin working immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data Exploration: Setting Up Grid Search\n",
    "\n",
    "As our first exploratory exercise, we would like to run __Grid Search__ over a broad search space to incrementally and exhaustively test many combinations of parameters and evaluate the resulting concepts.\n",
    "\n",
    "We are looking for two specific characteristics within results as judged by the user: coherence and recall (i.e. human-understandable concepts that are distinct from one another)\n",
    "\n",
    "Open up prosper_config.json in either NotePad or your IDE and edit the parameters as desired or simply run the default configurations (seen below). We specify the output directory as 'test'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\n",
    "    \"dataset\": \"prosper_loan\",\n",
    "    \"gpus\": [0],\n",
    "    \"max_threads\": 1,\n",
    "    \"out_dir\": \"test\",\n",
    "    \"dataset_params\" : {\n",
    "        \"window_size\": [2, 4],\n",
    "        \"vocab_size\": [5000],\n",
    "        \"min_df\": [0.01, 0.001],\n",
    "        \"max_df\": [0.4, 0.6, 0.8]\n",
    "    },\n",
    "    \"fcm_params\": {\n",
    "        \"inductive\": [true],\n",
    "        \"inductive_dropout\": [0.01, 0.1, 0.2],\n",
    "        \"embed_dim\": [50, 200],\n",
    "        \"hidden_size\": [100],\n",
    "        \"num_layers\": [1],\n",
    "        \"nnegs\": [10, 20],\n",
    "        \"nconcepts\": [7],\n",
    "        \"lam\": [1, 10, 20, 100],\n",
    "        \"rho\": [10, 100, 1000],\n",
    "        \"eta\": [1, 10, 100, 500]\n",
    "    },\n",
    "    \"fit_params\": {\n",
    "        \"lr\": [0.01],\n",
    "        \"nepochs\": [30],\n",
    "        \"pred_only_epochs\": [0, 5, 10],\n",
    "        \"batch_size\": [5000],\n",
    "        \"grad_clip\": [1000]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following command in command line. Grid search will begin loading in the data and training. Please wait until the process has finished before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! fcm grid-search ../configs/prosper_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Evaluating Results\n",
    "\n",
    "Once the grid search is completed, we can now proceed to view the results. If interested, review the results.csv in the grid search directory to review performance metrics of each hyperparameter configuration in the search space.\n",
    "\n",
    "To review results, we will use the __Visualize__ feature. Run the command in command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! fcm visualize ../grid_search/prosper_loan/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the address to the development server and paste it into your web browser search bar. Remember to append '/viewer/' to the end of the address. The input should appear as (or similar to) 'http://127.0.0.1:8000/viewer/'.\n",
    "\n",
    "The visualization interface will appear, showing results from each of the configurations in the search space. In the example below, we expand the concept word set to gain a broader understanding of concepts found in the data. By default, FCM extracts 10 per concept. This feature is currently not available but will be implemented in a future update. [UMass measure](http://qpleple.com/topic-coherence-to-evaluate-topic-models/) \n",
    "\n",
    "![viz](img/3.3_visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer observation, many of the topic results seen above are generally uninformative as we cannot intuitively describe the concepts based on the words extracted.\n",
    "\n",
    "Note that in these configurations, topics appear to overlap and concept-describing words are often too scattered to describe a single coherent idea. These words cannot clearly or easily inform us about potential constructs to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Hyperparameter Tuning\n",
    "To address the above issues, we must perform hyperparameter tuning. Recall that FCM utilizes the __tf-idf__ mechanism. In order to extract coherent, distinct concepts, we must adjust __tf__ and __df__.\n",
    "\n",
    "Filtering can be performed at two stages: __Grid Search__ and __Visualize__. While __Grid Search__ can help obtain better baseline results, we recommend using the __Visualize__ interface to filter and obtain good results from the baseline search space produced by __Grid Search__.\n",
    "\n",
    "__Tf__ and __df__ of concept words can be filtered in the visualization interface via the filtering slider. __Df__ of the entire text vocabulary can be adjusted in __Grid Search__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify suitable df parameters across the entire text vocabulary, we run __Grid Search__ to test different minimum and maximum df parameters in the configuration file holding all else equal. Then, we use the __Visualize__ feature to manually evaluate the concept results from the broad exploratory set, adjusting the filters and observing the effect on concept coherence and recall. Once we identify several informative configurations, we will use the df parameters associated with those configurations in consequent __Grid Search__ runs.\n",
    "\n",
    "For example, we set the minimum and maximum df parameters in the configuration file to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"min_df\": [0.01, 0.1, 0.2, 0.3],\n",
    "\"max_df\": [0.4, 0.6, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remember__: the df parameter in __Grid Search__ applies to the entire text vocabulary while the df filter in __Visualize__ only applies to the extracted concept words.\n",
    "\n",
    "With this specific dataset, we find that constraining the df range slightly (0.1 to 0.8, for example) in the configuration file greatly improves the recall and coherence within and across concepts as evaluated by human judgement, i.e. concepts tend to be more informative and distinct from one another.\n",
    "\n",
    "After evaluating concept coherence from the initial exploratory configurations, we can systematically test other parameter configurations via __Grid Search__ holding the user-selected df parameters constant to find suitable hyperparameters that produce informative concepts.\n",
    "\n",
    "This process involves re-running __Grid Search__ numerous times while testing a range of values one parameter at a time holding all else equal. Review the results and identify the specific configurations that produce better results as judged by the user. Use those parameters and iterate over the next parameter.\n",
    "\n",
    "Following these steps will help build intuition as to what parameters work well. Once you have a grasp on effective hyperparameters, you can run focused search spaces with narrow parameter sets to extract more informative results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, here is a sample result from a poorly tuned run with arbitrary hyperparameters.\n",
    "\n",
    "![gs_bad](img/4.5_gs_bad.png)\n",
    "\n",
    "Notice how the concept words tend to overlap and generally express similar ideas.\n",
    "\n",
    "Next, with well-tuned df hyperparameters, holding all else equal, we obtain more informative baseline results as follows:\n",
    "\n",
    "![gs_good](img/4.5_gs_good.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Filtering Results\n",
    "\n",
    "Once you have made the appropriate adjustments to the configuration file, rerun __Grid Search__, now specifying a different output directory. In this example, we use 'run0' instead of 'test' as our directory. Execute the __Grid Search__ command again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! fcm grid-search ../configs/prosper_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result directories will be generated in accordance with the new output directory that you specify.\n",
    "\n",
    "Now, we can visualize our new results with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! fcm visualize ../grid_search/prosper_loan/run0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the same procedure to navigate to the __Visualize__ interface.\n",
    "\n",
    "Again, experiment with the filtering options in the visualization interface until you identify interesting patterns. The process is iterative and experimental in nature - keep adjusting parameters and filters to find coherent, distinct concepts!\n",
    "\n",
    "After several iterations of this process, we arrive at a __Grid Search__ space that highlights potential constructs that we can reasonably hypothesize are correlated with loan default. Notice how the concept-describing words are generally understandable and distinct and, upon closer observation, can highlight intuitive constructs like mentions of family and personal expenses.\n",
    "\n",
    "In consequent __Grid Search__ runs, we now have a baseline understanding of what constructs may be relevant and can substantiate our hypothesized concepts. Keep these in mind as you iterate through search spaces.\n",
    "\n",
    "![final](img/4.6_prosper_vis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another search space, we identify one configuration using the filtering features in __Visualize__ that highlights particularly coherent and distinct concepts like family, school and employment, personal expenses, etc.\n",
    "\n",
    "Without filtering, we observe the following concepts still with some degree of overlap:\n",
    "\n",
    "![bad](img/4.6_vis_bad.png)\n",
    "\n",
    "Once we apply filters, we obtain the following results which are much cleaner and more informative. We manually reorder the words to highlight the top words that suggest interesting constructs to investigate further.\n",
    "\n",
    "Note that FCM will not automatically reorder words in this manner.\n",
    "\n",
    "![good](img/4.6_prosper_final.png)\n",
    "\n",
    "These results pose an interesting hypothesis and can serve as the basis for further causal investigation. With these results, a manager or researcher can exercise domain expertise and apply involved causal inference methods to validate the hypothesis and ultimately extract business-relevant insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Closing Comments\n",
    "\n",
    "With a methodical approach to data exploration and results filtering, we successfully identify several interesting constructs from our test dataset for further causal investigation.\n",
    "\n",
    "Given that managers and researchers are often faced with sifting through volumes of unstructured data to extract patterns, FCM CLI can be an essential tool to obtain data-driven insights that users can inspect further to identify meaningful vs. spurious outcomes. A properly tuned FCM CLI can help you extract coherent concepts and build intuition towards insights about real-world phenomena.\n",
    "\n",
    "Thank you for using FCM CLI! Please reach out to the team with any inquiries, feedback, or suggestions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}